{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3bdf26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 16:38:53.882838: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# NOTE: This machine learning program is for predicting TC formation, using\n",
    "#       input dataset in the NETCDF format. The program treats different\n",
    "#       2D input fields as different channels of an image. This specific\n",
    "#       program requires a set of 12 2D-variables (12-channel image) and\n",
    "#       consists of three stages\n",
    "#       - Stage 1: reading NETCDF input and generating (X,y) data with a\n",
    "#                  given image sizes, which are then saved by pickle;\n",
    "#       - Stage 2: import the saved pickle (X,y) pair and build a CNN model\n",
    "#                  with a given training/validation ratio, and then save\n",
    "#                  the train model under tcg_CNN.model.\n",
    "#       - Stage 3: import the trained model from Stage 2, and make a list\n",
    "#                  of prediction from normalized test data.\n",
    "#\n",
    "# INPUT: This Stage 2 script requires two specific input datasets that are\n",
    "#        generated from Step 1, including\n",
    "#        1. tcg_X.pickle: data contains all images of yes/no TCG events, each\n",
    "#           of these images must have 12 channels\n",
    "#        2. tcg_y.pickle: data contains all labels of each image (i.e., yes\n",
    "#           or no) of TCG corresponding to each data in X.\n",
    "#\n",
    "#        Remarks: Note that each channel must be normalized separealy. Also\n",
    "#        the script requires a large memory allocation. So users need to have\n",
    "#        GPU version to run this.\n",
    "#\n",
    "# OUTPUT: A CNN model built from Keras saved under tcg_CNN.model\n",
    "#\n",
    "# HIST: - 27, Oct 22: Created by CK\n",
    "#       - 01, Nov 22: Modified to include more channels\n",
    "#       - 17, Nov 23: cusomize it for jupiter notebook\n",
    "#\n",
    "# AUTH: Chanh Kieu (Indiana University, Bloomington. Email: ckieu@iu.edu)\n",
    "#\n",
    "#==========================================================================\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80fbaaa0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape of the X features data:  (24, 30, 30, 12)\n",
      "Input shape of the y label data:  (24,)\n",
      "Number of input channel extracted from X is:  12\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# read in data output from Part 1\n",
    "#\n",
    "pickle_in = open(\"tcg_X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "pickle_in = open(\"tcg_y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "y = np.array(y)\n",
    "number_channels=X.shape[3]\n",
    "print('Input shape of the X features data: ',X.shape)\n",
    "print('Input shape of the y label data: ',y.shape)\n",
    "print('Number of input channel extracted from X is: ',number_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0ad37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 128 3\n",
      "0 128 5\n",
      "0 256 3\n",
      "0 256 5\n",
      "5 128 3\n",
      "5 128 5\n",
      "5 256 3\n",
      "5 256 5\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# test a range of loop for hyperparameter tuning. This is a test and have no impact to the rest of the\n",
    "# code. \n",
    "#\n",
    "dense_layers = [0, 5]\n",
    "layer_sizes = [128, 256]\n",
    "conv_layers = [3, 5]\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            print(dense_layer, layer_size, conv_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2aaa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running configuration:  3-conv-128-nodes-0-dense-1676929414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 16:43:36.084852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-20 16:43:39.604912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30987 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 16:43:46.777588: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 15s 15s/step - loss: 3230.0630 - accuracy: 0.4286 - val_loss: 31261.2871 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 13403.7676 - accuracy: 0.5714 - val_loss: 17684.6777 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7580.3325 - accuracy: 0.5714 - val_loss: 132.5553 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 53.1010 - accuracy: 0.5714 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8873.1826 - accuracy: 0.4286 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8224.6494 - accuracy: 0.4286 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4777.2402 - accuracy: 0.4286 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1081.6517 - accuracy: 0.4286 - val_loss: 3179.2068 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1359.2845 - accuracy: 0.5714 - val_loss: 4755.3008 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2038.9260 - accuracy: 0.5714 - val_loss: 4450.5425 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1909.7078 - accuracy: 0.5714 - val_loss: 3253.7864 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1396.6532 - accuracy: 0.5714 - val_loss: 1426.9486 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 615.1607 - accuracy: 0.5714 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 405.8245 - accuracy: 0.4286 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 616.0121 - accuracy: 0.4286 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 202.1403 - accuracy: 0.4286 - val_loss: 623.2154 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 274.9238 - accuracy: 0.5714 - val_loss: 591.5056 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 259.0502 - accuracy: 0.5714 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 27.5109 - accuracy: 0.5238 - val_loss: 163.8972 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 73.6111 - accuracy: 0.5714 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 125.2884 - accuracy: 0.4286 - val_loss: 127.2627 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 59.6475 - accuracy: 0.5714 - val_loss: 7.1432 - val_accuracy: 0.6667\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.1935 - accuracy: 0.8571 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 88.9315 - accuracy: 0.4286 - val_loss: 202.5039 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 89.5563 - accuracy: 0.5714 - val_loss: 139.5265 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 62.1065 - accuracy: 0.5714 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 167.6977 - accuracy: 0.4286 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 92.8558 - accuracy: 0.4286 - val_loss: 330.6398 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 142.1857 - accuracy: 0.5714 - val_loss: 482.8828 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 207.8931 - accuracy: 0.5714 - val_loss: 288.4447 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tcg_CNN.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tcg_CNN.model/assets\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# build a range of CNN models with different number of dense layers, layer sizes\n",
    "# convolution layers to optimize the performance\n",
    "#\n",
    "dense_layers = [0]\n",
    "layer_sizes = [128]\n",
    "conv_layers = [3]\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print('--> Running configuration: ',NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:],data_format=\"channels_last\"))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            for _ in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            tensorboard = TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            model.fit(X, y, batch_size=90, epochs=30, validation_split=0.1, callbacks=[tensorboard])\n",
    "#\n",
    "# save the model for the final step\n",
    "#\n",
    "model.save('tcg_CNN.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca670d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
