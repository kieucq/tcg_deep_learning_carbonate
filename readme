NOTE: This directory contains several different deep-learning models for TC formation detection
      from a given binary reanalysis dataset, which are centered on TC locations from the best
      track data. This input data must be created in advance, which can be generated by using 
      e.g., the scripts under the utils location.

      Note that there are two types of input data: one is highly unbalanced data with the number
      of TCG events much less than the total number of days with no TCG. This dataset is created
      by using an external script "extract_environment_features_from_ncep.py" with renalysis +
      best track data. The second type if input data is the pos/neg pair that is created centered
      on the best track genesis location with grid size of 30x30 degree, which is used in this 
      Git workflow. The script for producing this is "create_tc_binary_classification_ncep.py".
      These external scripts can be found in the utils dir, or it can be created easily. See the 
      dir "data" for the structure of the input data.

      For the type of large domain input using "extract_environment_features_from_ncep.py", note
      also that this script will produce a single input dir with both pos/neg data combined. The
      pos/neg label is contained in the csv file included within this domain. One can re-structure
      this input data dir by using script "create_tc_binary_ncep_chanh.ipynb", which will generate
      two separate dirs for pos/neg similar to typical Keras dataset input structure. 

      Remark: the deep-learning architechures contained in this Git are specifically designed for
      12 channel input. These are hard-wired in all scripts, and one must re-generate all input
      data if different channels/variables are used. 

DIR STRCUTURE:
1. utils: This dir contains all utilities that can be used to extract NCEP/WRF output to a
      binary input data structure for ML training processes. This step must be done first
      if moving to a new system. These utilities are copied from Quan Nguyen's research.

2. data: data structure for training/testing of TCG prediction problem

3. CNN-sequential: a simple CNN sequential model that is built from scratch, with only a few
      convolutionl layers. This model will output a bunch of save-best models that are varying
      with different number of dense layers, filters, and kernel sizes.

4. CNN-functional: a more tuned CNN functionla model that has some additional options for
      the accuracy metric. Similar to CNN-sequential model, this model also has a range of
      model save-best outputs with different number of dense layers, filters, and kernel
      sizes. There are two versions for this architecture, one is using the binary_accuracy
      and the other use the F1-score as a metric   

5. CNN-augmentation: the same as CNN-functional but with augmentation and dropout added. Not
      much improvement overall.

6. ResNet: This dir contains a deep ResNet model with 20, 22, and 40 convolutional layers, using
      identity block/convolutional blocks as in Andrew Ng's lectures. It will output however
      a single save-best model. Work fine for ResNet-20 but does not perform as well as a 
      simple CNN-functional.

7. InceptionV3: trasnfer learning model using InceptionV3 model, but it does not work currently
      as the Inception model requires 3-channel input data, while the TCG data are 12-channel
      arrays. 

8. diagnostics: this dir contains all visualization scripts that can help understand how CNN works
      using IG, heatmap, ...

9. preprocess: this dir contains important scripts that generate binary dataset for either storm-following
      or large static domains from NCEP renalysis or WRF downscaling

AUTHOR: (C) Chanh Kieu, ckieu@indiana.edu. Open source licences.
